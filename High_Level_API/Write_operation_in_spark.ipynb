{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Write Operation in Spark\n",
        "\n",
        "* consider as action when we write some dataframe , but what we meaning by write is save dataframe after processing and cleaning"
      ],
      "metadata": {
        "id": "Xb6rjVNWiMtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sYTqyxiThLIH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Write_in_Spark\") \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Achraf\", \"Rabat\", \"2024-01-10\", True),\n",
        "    (2, \"Sara\", \"Casablanca\", \"2024-02-12\", False),\n",
        "    (3, \"Omar\", \"Fes\", \"2024-03-05\", True),\n",
        "    (4, \"Youssef\", \"Tanger\", \"2024-04-19\", True),\n",
        "    (5, \"Lina\", \"Marrakech\", \"2024-05-01\", False),\n",
        "    (6, \"Hassan\", \"Agadir\", \"2024-01-27\", True),\n",
        "    (7, \"Nadia\", \"Tetouan\", \"2024-03-14\", False),\n",
        "    (8, \"Khalid\", \"Oujda\", \"2024-02-28\", True),\n",
        "    (9, \"Salma\", \"Kenitra\", \"2024-04-07\", True),\n",
        "    (10, \"Mehdi\", \"Rabat\", \"2024-05-09\", False),\n",
        "]\n",
        "columns = [\"customer_id\", \"name\", \"city\", \"order_date\", \"is_active\"]\n"
      ],
      "metadata": {
        "id": "L_Bl4xdvhhTx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFJZRs_8huyT",
        "outputId": "3f631cdc-d503-4b8c-9492-b15153cc1310"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----------+----------+---------+\n",
            "|customer_id|   name|      city|order_date|is_active|\n",
            "+-----------+-------+----------+----------+---------+\n",
            "|          1| Achraf|     Rabat|2024-01-10|     true|\n",
            "|          2|   Sara|Casablanca|2024-02-12|    false|\n",
            "|          3|   Omar|       Fes|2024-03-05|     true|\n",
            "|          4|Youssef|    Tanger|2024-04-19|     true|\n",
            "|          5|   Lina| Marrakech|2024-05-01|    false|\n",
            "|          6| Hassan|    Agadir|2024-01-27|     true|\n",
            "|          7|  Nadia|   Tetouan|2024-03-14|    false|\n",
            "|          8| Khalid|     Oujda|2024-02-28|     true|\n",
            "|          9|  Salma|   Kenitra|2024-04-07|     true|\n",
            "|         10|  Mehdi|     Rabat|2024-05-09|    false|\n",
            "+-----------+-------+----------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Operation"
      ],
      "metadata": {
        "id": "YUXdlD6BiizY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.format('csv').option('header','true').save(\"/content/clean.csv\")\n",
        "# can add delimiter option\n",
        "# df.write.format('csv').option('header','true').optin(\"delimiter\",'true').save(\"/content/clean.csv\")"
      ],
      "metadata": {
        "id": "_KGh4Tdsin5v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYXvunHCjJaR",
        "outputId": "753f5af0-07d4-4d52-9596-65805070c284"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so here we have two partition let's try use `repartition()` function to decresss number of partition into 1"
      ],
      "metadata": {
        "id": "c4ons9bSjRK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.repartition(1).write.format('csv').option('header','true').save(\"/content/clean_data_1.csv\")"
      ],
      "metadata": {
        "id": "c_odWgiBjgTo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's try read data , and see number of partition"
      ],
      "metadata": {
        "id": "jAxaRvTuk6vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean.csv ----> 2 partition\n",
        "df_2_partition=spark.read.format('csv').option('header','true').option('inferschema','true').load('/content/clean.csv')"
      ],
      "metadata": {
        "id": "T4evBqkwk_Df"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2_partition.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSLuKtoqlRWA",
        "outputId": "a82becac-6cbe-4dce-946a-abf0345db58f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_data_1.csv -----> 1 partition\n",
        "df_1_partition=spark.read.format('csv').option('header','true').option('inferschema','true').load('/content/clean_data_1.csv')\n",
        "df_1_partition.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfIEyQQild9y",
        "outputId": "b558f7f7-3d22-43c6-a315-a6be2b1fc1e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_partition.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Fd-HjlmBCN",
        "outputId": "cfb6b36c-e957-4e60-bfc8-6bc840dcb834"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- is_active: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}